{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.5.0\n",
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import transformers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "PRETRAINED_MODEL_NAME = \"hfl/chinese-bert-wwm\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "print(\"PyTorch 版本：\", torch.__version__)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda:  10.2\n",
      "torch.__version__:  1.5.0\n",
      "transformers.__version__:  2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.version.cuda: \",torch.version.cuda)\n",
    "print(\"torch.__version__: \",torch.__version__)\n",
    "print(\"transformers.__version__: \",transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：包含 [CLS]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "class NLPDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, tokenizer, path, maxLength):\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv( path, sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'postive': 0, 'negtive': 1}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        self.articleLegth = maxLength\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        news_ID, text, label = self.df.iloc[idx, :].values\n",
    "        label_tensor = torch.tensor(label)\n",
    "        news_ID_tensor = torch.tensor(news_ID)   \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        lens = self.articleLegth-len(tokens)-1\n",
    "        for I in range(lens):\n",
    "            #padding 文章長度不足的部分\n",
    "            tokens += [\"[PAD]\"]\n",
    "        word_pieces += tokens\n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([1] * len(word_pieces), \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, news_ID_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "        \n",
    "    if samples[0][3] is not None:\n",
    "        news_ID_ids = torch.stack([s[3] for s in samples])\n",
    "    else:\n",
    "        news_ID_ids = None    \n",
    "    # zero pad 到同一序列長度\n",
    "\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "\n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids, news_ID_ids\n",
    "\n",
    "\n",
    "def modelPredictions(model, dataloader, compute_acc=True):\n",
    "    predictions = None\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    news_ID_wrong = []\n",
    "    wrongTotal = 0\n",
    "    print(\"start model predit!! \",\"\\n\")\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "#             別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "#             且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors, yTrue, news_ID = data[:5]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "#                 y_true += labels.size(0)\n",
    "                if (y_true == []):\n",
    "                    y_true = labels\n",
    "                else:\n",
    "                    y_true = torch.cat((y_true,labels), dim=0)\n",
    "                if (y_pred == []):\n",
    "                    y_pred = pred\n",
    "                else:\n",
    "                    y_pred = torch.cat((y_pred,pred), dim=0)\n",
    "                    \n",
    "            for I in range(tokens_tensors.shape[0]):\n",
    "#                 if(yTrue[I] != pred[I] or yTrue[I]==1):\n",
    "                if(yTrue[I] != pred[I] or pred[I] ==1 ):\n",
    "                    temp = tokens_tensors[I]\n",
    "                    tokens = tokenizer.convert_ids_to_tokens(temp.tolist())  \n",
    "                    news_ID_wrong = np.append(news_ID_wrong,Variable(news_ID).cpu().numpy()[I])\n",
    "                    combined_text = \"\".join(tokens)  \n",
    "                    if (combined_text.find(\"[PAD]\")!=-1):\n",
    "                        combined_text = combined_text[:combined_text.find(\"[PAD]\")]\n",
    "                    print(f\"\"\" true ：{yTrue[I]} pred  ：{pred[I]} news_ID  ：{news_ID[I]}\n",
    "                    --------------------\n",
    "                    \"\"\")\n",
    "#                     print(\"context:\", df_all.loc[df_all.loc[:,'news_ID']==news_ID[I],'URL_CONTENT_new'].values[0])\n",
    "                    print(\"context:\", combined_text[5:])\n",
    "                    print(\"\\n\")\n",
    "                    wrongTotal += 1\n",
    "        print(\"wrongTotal: \",wrongTotal)\n",
    "        print(\"news_ID_wrong: \",news_ID_wrong.astype(int))\n",
    "        \n",
    "    if compute_acc:\n",
    "        tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "\n",
    "        epsilon = 1e-7\n",
    "\n",
    "        precision = tp / (tp + fp + epsilon)\n",
    "        recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "        f1 = 2* (precision*recall) / (precision + recall + epsilon)  \n",
    "        print(\"fi score ：%.3f, recall ：%.3f, precision  ：%.3f\"%(f1,recall,precision)) \n",
    "        \n",
    "def mainDataProcess(BATCH_SIZE, path, maxLength):\n",
    "    dataSet = NLPDataset(tokenizer=tokenizer, path = path, maxLength = maxLength)  \n",
    "    loader = DataLoader(dataSet, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 1\n",
    "    path = \"data/\"\n",
    "    maxLength = 512\n",
    "    PATH = \"model/model_512_20200730.pkl\"\n",
    "    dataloader = mainDataProcess(BATCH_SIZE, path + 'all_data_20200730.tsv', maxLength)\n",
    "    model = torch.load(PATH)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    model = model.to(device)\n",
    "    startTime = datetime.datetime.now()\n",
    "    modelPredictions(model, dataloader)\n",
    "    endTime = datetime.datetime.now()\n",
    "    print(\"time:\", endTime-startTime)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
