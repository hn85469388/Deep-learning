{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.5.0\n",
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"hfl/chinese-bert-wwm\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：包含 [CLS]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    " \n",
    "\n",
    "class NLPDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer,maxLength, path):\n",
    "        assert mode in [\"train\", \"test\", \"all data\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(path, sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.maxLength = maxLength\n",
    "        self.label_map = {'postive': 0, 'negtive': 1}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        news_ID, text, label = self.df.iloc[idx, :].values\n",
    "        label_tensor = torch.tensor(label)\n",
    "        news_ID_tensor = torch.tensor(news_ID)   \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        lens = self.maxLength-len(tokens)-1\n",
    "        for I in range(lens):\n",
    "            #padding 文章長度不足的部分\n",
    "            tokens += [\"[PAD]\"]\n",
    "        word_pieces += tokens\n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([1] * len(word_pieces), \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, news_ID_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLength = 512\n",
    "\n",
    "pathTrain = \"D:/contest/E_SUN_Bank_NLP/bertNER-master/AML data/total/train_all_data_20200730.tsv\"\n",
    "pathTest = \"D:/contest/E_SUN_Bank_NLP/bertNER-master/AML data/total/test_all_data_20200730.tsv\"\n",
    "pathAll = \"D:/contest/E_SUN_Bank_NLP/bertNER-master/AML data/total/all_data_20200730.tsv\"\n",
    "\n",
    "trainset = NLPDataset(\"train\", tokenizer=tokenizer, maxLength = maxLength, path = pathTrain)\n",
    "testset = NLPDataset(\"test\", tokenizer=tokenizer, maxLength = maxLength, path = pathTest)\n",
    "allset = NLPDataset(\"all data\", tokenizer=tokenizer, maxLength = maxLength, path = pathAll)\n",
    "\n",
    "trainData = pd.read_csv(pathTrain, sep=\"\\t\").fillna(\"\")\n",
    "testData = pd.read_csv(pathTest, sep=\"\\t\").fillna(\"\")\n",
    "allDAta = pd.read_csv(pathAll, sep=\"\\t\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "        \n",
    "    if samples[0][3] is not None:\n",
    "        news_ID_ids = torch.stack([s[3] for s in samples])\n",
    "    else:\n",
    "        news_ID_ids = None    \n",
    "    # zero pad 到同一序列長度\n",
    "\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "\n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids, news_ID_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 5 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 5\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)\n",
    "\n",
    "all_loader = DataLoader(allset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "def get_predictions(model, dataloader, dataName, size, compute_acc=False):\n",
    "    predictions = None\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    pbar = pkbar.Pbar(name= dataName + ' predict~!!', target=size)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        i=0\n",
    "        for data in dataloader:\n",
    "            pbar.update(i)\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors, yTrue = data[:4]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            \n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "#                 y_true += labels.size(0)\n",
    "                if (y_true == []):\n",
    "                    y_true = labels\n",
    "                else:\n",
    "                    y_true = torch.cat((y_true,labels), dim=0)\n",
    "                if (y_pred == []):\n",
    "                    y_pred = pred\n",
    "                else:\n",
    "                    y_pred = torch.cat((y_pred,pred), dim=0)\n",
    "#                 correct += (pred == labels).sum().item()\n",
    "#                 y_pred += pred \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "            i+=1\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    if compute_acc:\n",
    "        tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "\n",
    "        epsilon = 1e-7\n",
    "\n",
    "        precision = tp / (tp + fp + epsilon)\n",
    "        recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "        f1 = 2* (precision*recall) / (precision + recall + epsilon)  \n",
    "        return predictions, f1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "torch.cuda.empty_cache()\n",
    "PRETRAINED_MODEL_NAME = \"hfl/chinese-bert-wwm\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model/model_512_20200722.pkl\"\n",
    "model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train~!!\n",
      "Epoch: 1/5000\n",
      "827/826  [==============================] - 292.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.9s\n",
      "\n",
      "[epoch 1] train loss: 68.608, test f1: 0.932\n",
      "Epoch: 2/5000\n",
      "827/826  [==============================] - 605.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 2] train loss: 42.542, test f1: 0.940\n",
      "refresh and save best model,train loss: 42.542, best test f1:0.940\n",
      "Epoch: 3/5000\n",
      "827/826  [==============================] - 919.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.6s\n",
      "\n",
      "[epoch 3] train loss: 35.754, test f1: 0.935\n",
      "Epoch: 4/5000\n",
      "827/826  [==============================] - 1228.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 4] train loss: 33.251, test f1: 0.939\n",
      "Epoch: 5/5000\n",
      "827/826  [==============================] - 1536.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 5] train loss: 27.252, test f1: 0.952\n",
      "refresh and save best model,train loss: 27.252, best test f1:0.952\n",
      "Epoch: 6/5000\n",
      "827/826  [==============================] - 1845.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 6] train loss: 24.204, test f1: 0.942\n",
      "Epoch: 7/5000\n",
      "827/826  [==============================] - 2153.0s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 7] train loss: 19.878, test f1: 0.955\n",
      "refresh and save best model,train loss: 19.878, best test f1:0.955\n",
      "Epoch: 8/5000\n",
      "827/826  [==============================] - 2461.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 8] train loss: 14.824, test f1: 0.942\n",
      "Epoch: 9/5000\n",
      "827/826  [==============================] - 2769.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 9] train loss: 10.995, test f1: 0.950\n",
      "Epoch: 10/5000\n",
      "827/826  [==============================] - 3077.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 10] train loss: 8.516, test f1: 0.937\n",
      "Epoch: 11/5000\n",
      "827/826  [==============================] - 3385.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 11] train loss: 5.934, test f1: 0.931\n",
      "Epoch: 12/5000\n",
      "827/826  [==============================] - 3693.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 12] train loss: 4.133, test f1: 0.943\n",
      "Epoch: 13/5000\n",
      "827/826  [==============================] - 4001.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 13] train loss: 2.874, test f1: 0.946\n",
      "Epoch: 14/5000\n",
      "827/826  [==============================] - 4309.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 14] train loss: 2.650, test f1: 0.960\n",
      "refresh and save best model,train loss: 2.650, best test f1:0.960\n",
      "Epoch: 15/5000\n",
      "827/826  [==============================] - 4618.4s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 15] train loss: 1.332, test f1: 0.930\n",
      "Epoch: 16/5000\n",
      "827/826  [==============================] - 4926.7s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 16] train loss: 1.627, test f1: 0.943\n",
      "Epoch: 17/5000\n",
      "827/826  [==============================] - 5235.0s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 17] train loss: 0.704, test f1: 0.955\n",
      "Epoch: 18/5000\n",
      "827/826  [==============================] - 5543.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 18] train loss: 1.652, test f1: 0.938\n",
      "Epoch: 19/5000\n",
      "827/826  [==============================] - 5851.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 19] train loss: 0.574, test f1: 0.951\n",
      "Epoch: 20/5000\n",
      "827/826  [==============================] - 6159.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 20] train loss: 0.580, test f1: 0.941\n",
      "Epoch: 21/5000\n",
      "827/826  [==============================] - 6468.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 21] train loss: 0.329, test f1: 0.946\n",
      "Epoch: 22/5000\n",
      "827/826  [==============================] - 6776.9s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 22] train loss: 0.959, test f1: 0.947\n",
      "Epoch: 23/5000\n",
      "827/826  [==============================] - 7085.7s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 23] train loss: 0.242, test f1: 0.942\n",
      "Epoch: 24/5000\n",
      "827/826  [==============================] - 7394.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 24] train loss: 0.161, test f1: 0.937\n",
      "Epoch: 25/5000\n",
      "827/826  [==============================] - 7702.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 25] train loss: 0.202, test f1: 0.951\n",
      "Epoch: 26/5000\n",
      "827/826  [==============================] - 8010.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 26] train loss: 1.005, test f1: 0.940\n",
      "Epoch: 27/5000\n",
      "827/826  [==============================] - 8318.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 27] train loss: 3.331, test f1: 0.960\n",
      "Epoch: 28/5000\n",
      "827/826  [==============================] - 8626.4s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 28] train loss: 0.117, test f1: 0.956\n",
      "Epoch: 29/5000\n",
      "827/826  [==============================] - 8934.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 29] train loss: 0.101, test f1: 0.955\n",
      "Epoch: 30/5000\n",
      "827/826  [==============================] - 9243.0s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 30] train loss: 0.144, test f1: 0.946\n",
      "Epoch: 31/5000\n",
      "827/826  [==============================] - 9551.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 31] train loss: 0.094, test f1: 0.946\n",
      "Epoch: 32/5000\n",
      "827/826  [==============================] - 9859.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 32] train loss: 0.065, test f1: 0.951\n",
      "Epoch: 33/5000\n",
      "827/826  [==============================] - 10168.0s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 33] train loss: 0.059, test f1: 0.960\n",
      "refresh and save best model,train loss: 0.059, best test f1:0.960\n",
      "Epoch: 34/5000\n",
      "827/826  [==============================] - 10476.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 34] train loss: 0.279, test f1: 0.947\n",
      "Epoch: 35/5000\n",
      "827/826  [==============================] - 10784.4s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 35] train loss: 0.900, test f1: 0.951\n",
      "Epoch: 36/5000\n",
      "827/826  [==============================] - 11092.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 36] train loss: 0.033, test f1: 0.946\n",
      "Epoch: 37/5000\n",
      "827/826  [==============================] - 11400.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 37] train loss: 0.042, test f1: 0.951\n",
      "Epoch: 38/5000\n",
      "827/826  [==============================] - 11708.4s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 38] train loss: 0.038, test f1: 0.942\n",
      "Epoch: 39/5000\n",
      "827/826  [==============================] - 12016.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 39] train loss: 0.030, test f1: 0.951\n",
      "Epoch: 40/5000\n",
      "827/826  [==============================] - 12324.9s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 40] train loss: 0.027, test f1: 0.947\n",
      "Epoch: 41/5000\n",
      "827/826  [==============================] - 12633.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 41] train loss: 1.605, test f1: 0.960\n",
      "Epoch: 42/5000\n",
      "827/826  [==============================] - 12941.7s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 42] train loss: 1.397, test f1: 0.951\n",
      "Epoch: 43/5000\n",
      "827/826  [==============================] - 13249.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 43] train loss: 0.026, test f1: 0.942\n",
      "Epoch: 44/5000\n",
      "827/826  [==============================] - 13558.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 44] train loss: 0.029, test f1: 0.955\n",
      "Epoch: 45/5000\n",
      "827/826  [==============================] - 13866.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 45] train loss: 0.026, test f1: 0.947\n",
      "Epoch: 46/5000\n",
      "827/826  [==============================] - 14174.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 46] train loss: 0.027, test f1: 0.933\n",
      "Epoch: 47/5000\n",
      "827/826  [==============================] - 14482.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 47] train loss: 0.022, test f1: 0.951\n",
      "Epoch: 48/5000\n",
      "827/826  [==============================] - 14790.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 48] train loss: 0.020, test f1: 0.951\n",
      "Epoch: 49/5000\n",
      "827/826  [==============================] - 15098.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 49] train loss: 0.138, test f1: 0.936\n",
      "Epoch: 50/5000\n",
      "827/826  [==============================] - 15406.7s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 50] train loss: 0.017, test f1: 0.932\n",
      "Epoch: 51/5000\n",
      "827/826  [==============================] - 15714.9s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 51] train loss: 0.014, test f1: 0.927\n",
      "Epoch: 52/5000\n",
      "827/826  [==============================] - 16022.9s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 52] train loss: 0.014, test f1: 0.955\n",
      "Epoch: 53/5000\n",
      "827/826  [==============================] - 16331.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 53] train loss: 0.018, test f1: 0.906\n",
      "Epoch: 54/5000\n",
      "827/826  [==============================] - 16639.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 54] train loss: 0.012, test f1: 0.927\n",
      "Epoch: 55/5000\n",
      "827/826  [==============================] - 16947.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 55] train loss: 0.008, test f1: 0.960\n",
      "Epoch: 56/5000\n",
      "827/826  [==============================] - 17256.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 56] train loss: 0.007, test f1: 0.942\n",
      "Epoch: 57/5000\n",
      "827/826  [==============================] - 17564.3s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 57] train loss: 0.005, test f1: 0.937\n",
      "Epoch: 58/5000\n",
      "827/826  [==============================] - 17872.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 58] train loss: 0.004, test f1: 0.946\n",
      "Epoch: 59/5000\n",
      "827/826  [==============================] - 18180.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 59] train loss: 1.931, test f1: 0.951\n",
      "Epoch: 60/5000\n",
      "827/826  [==============================] - 18488.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 60] train loss: 0.008, test f1: 0.952\n",
      "Epoch: 61/5000\n",
      "827/826  [==============================] - 18796.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 61] train loss: 0.005, test f1: 0.951\n",
      "Epoch: 62/5000\n",
      "827/826  [==============================] - 19105.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 62] train loss: 0.005, test f1: 0.951\n",
      "Epoch: 63/5000\n",
      "827/826  [==============================] - 19413.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.6s\n",
      "\n",
      "[epoch 63] train loss: 0.007, test f1: 0.941\n",
      "Epoch: 64/5000\n",
      "827/826  [==============================] - 19720.9s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 64] train loss: 0.004, test f1: 0.941\n",
      "Epoch: 65/5000\n",
      "827/826  [==============================] - 20028.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 65] train loss: 0.136, test f1: 0.941\n",
      "Epoch: 66/5000\n",
      "827/826  [==============================] - 20336.7s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 66] train loss: 0.508, test f1: 0.932\n",
      "Epoch: 67/5000\n",
      "827/826  [==============================] - 20644.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 67] train loss: 0.003, test f1: 0.932\n",
      "Epoch: 68/5000\n",
      "827/826  [==============================] - 20952.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.7s\n",
      "\n",
      "[epoch 68] train loss: 0.004, test f1: 0.951\n",
      "Epoch: 69/5000\n",
      "827/826  [==============================] - 21260.6s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 69] train loss: 0.004, test f1: 0.936\n",
      "Epoch: 70/5000\n",
      "827/826  [==============================] - 21568.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 70] train loss: 0.003, test f1: 0.951\n",
      "Epoch: 71/5000\n",
      "827/826  [==============================] - 21876.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 71] train loss: 0.189, test f1: 0.928\n",
      "Epoch: 72/5000\n",
      "827/826  [==============================] - 22184.8s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 72] train loss: 0.004, test f1: 0.923\n",
      "Epoch: 73/5000\n",
      "827/826  [==============================] - 22492.7s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 73] train loss: 0.003, test f1: 0.924\n",
      "Epoch: 74/5000\n",
      "827/826  [==============================] - 22800.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 74] train loss: 0.005, test f1: 0.929\n",
      "Epoch: 75/5000\n",
      "827/826  [==============================] - 23108.4s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 75] train loss: 0.003, test f1: 0.933\n",
      "Epoch: 76/5000\n",
      "827/826  [==============================] - 23416.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 76] train loss: 1.373, test f1: 0.911\n",
      "Epoch: 77/5000\n",
      "827/826  [==============================] - 23724.4s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 77] train loss: 2.026, test f1: 0.934\n",
      "Epoch: 78/5000\n",
      "827/826  [==============================] - 24032.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 78] train loss: 0.004, test f1: 0.943\n",
      "Epoch: 79/5000\n",
      "827/826  [==============================] - 24340.1s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 79] train loss: 0.007, test f1: 0.947\n",
      "Epoch: 80/5000\n",
      "827/826  [==============================] - 24647.9s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 80] train loss: 0.005, test f1: 0.943\n",
      "Epoch: 81/5000\n",
      "827/826  [==============================] - 24955.7s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 81] train loss: 0.004, test f1: 0.951\n",
      "Epoch: 82/5000\n",
      "827/826  [==============================] - 25263.5s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 82] train loss: 0.003, test f1: 0.943\n",
      "Epoch: 83/5000\n",
      "827/826  [==============================] - 25571.2s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 83] train loss: 0.005, test f1: 0.937\n",
      "Epoch: 84/5000\n",
      "827/826  [==============================] - 25879.0s\n",
      "\n",
      "test predict~!!\n",
      "207/206  [==============================] - 20.8s\n",
      "\n",
      "[epoch 84] train loss: 1.310, test f1: 0.935\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "import pkbar\n",
    "\n",
    "startCount = 0\n",
    "count = 0\n",
    "bestTestAcc = 0\n",
    "preTestAcc = 0\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-7)\n",
    "\n",
    "EPOCHS = 5000  # 幸運數字\n",
    "pbar = pkbar.Pbar(name='Train~!!', target=len(trainData)/BATCH_SIZE)\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    print('Epoch: %d/%d' % (epoch + 1, EPOCHS))\n",
    "    i = 0\n",
    "    for data in trainloader:\n",
    "\n",
    "        pbar.update(i)\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels, news_ID = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    # 計算分類準確率\n",
    "#     _, train_f1 = get_predictions(model, trainloader, \"train\", len(trainData)/BATCH_SIZE, compute_acc=True)\n",
    "    _,test_f1 = get_predictions(model, testloader, \"test\", len(testData)/BATCH_SIZE, compute_acc=True)\n",
    "#     train_acc = Variable(train_f1).cpu().numpy()\n",
    "    test_acc = Variable(test_f1).cpu().numpy()\n",
    "    print('[epoch %d] train loss: %.3f, test f1: %.3f'\n",
    "          %(epoch + 1, running_loss, test_acc))\n",
    "\n",
    "    if (test_acc > 0.8 and startCount == 0):\n",
    "        startCount = 1\n",
    "        bestTestAcc = test_acc\n",
    "    if(startCount == 1 and bestTestAcc > test_acc):\n",
    "        count += 1\n",
    "    elif(startCount == 1 and bestTestAcc < test_acc):\n",
    "        PATH = \"model_512_20200730.pkl\"\n",
    "        torch.save(model, PATH)\n",
    "        bestTestAcc = test_acc\n",
    "        count = 0    \n",
    "        print(\"refresh and save best model,train loss: %.3f, best test f1:%.3f\"%( running_loss, bestTestAcc))\n",
    "\n",
    "    elif(startCount == 1 and bestTestAcc < test_acc):\n",
    "        PATH = \"model_512_20200730.pkl\"\n",
    "        torch.save(model, PATH)\n",
    "        bestTestAcc = test_acc\n",
    "        count = 0    \n",
    "        print(\"refresh and save best model, best test f1:%.3f\"%( bestTestAcc))\n",
    "    if (count > 50):\n",
    "        print(\"break\")\n",
    "        break\n",
    "#     print('[epoch %d] train loss: %.3f, train f1: %.3f, test f1: %.3f'\n",
    "#           %(epoch + 1, running_loss, train_acc, test_acc))\n",
    "\n",
    "#     if (test_acc > 0.8 and startCount == 0):\n",
    "#         startCount = 1\n",
    "#         bestTestAcc = test_acc\n",
    "#     if(startCount == 1 and bestTestAcc > test_acc):\n",
    "#         count += 1\n",
    "#     elif(startCount == 1 and bestTestAcc < test_acc):\n",
    "#         PATH = \"model_512_20200729.pkl\"\n",
    "#         torch.save(model, PATH)\n",
    "#         bestTestAcc = test_acc\n",
    "#         count = 0    \n",
    "#         print(\"refresh and save best model,train loss: %.3f,  train f1: %.3f, best test f1:%.3f\"%( running_loss, train_acc, bestTestAcc))\n",
    "#     if (count > 10):\n",
    "#         print(\"break\")\n",
    "#         break\n",
    "\n",
    "#     elif(startCount == 1 and bestTestAcc < test_acc):\n",
    "#         PATH = \"model_512_20200729.pkl\"\n",
    "#         torch.save(model, PATH)\n",
    "#         bestTestAcc = test_acc\n",
    "#         count = 0    \n",
    "#         print(\"refresh and save best model, train f1: %.3f, best test f1:%.3f\"%( train_acc, bestTestAcc))\n",
    "#     if (count > 10):\n",
    "#         print(\"break\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
